# Anti-Exploitation Training (v3.3)

## Problem Statement

The v3.2 model has learned a **degenerate strategy**:
- 0% fold rate
- 0% call rate
- 100% bet/raise every hand

This "hyper-aggression" strategy works against weak opponents but is **easily exploitable**.

### The Exploit

```
Exploiter vs Our Model:
┌────────────────────────────────────────────────────────────┐
│ Our model: Always bets/raises regardless of hand strength  │
│                                                            │
│ Exploiter strategy:                                        │
│   Strong hand (top 20%): Call/trap → Model bets into us    │
│   Weak hand (bottom 80%): Fold → Save chips                │
│                                                            │
│ Result: We win small pots (they fold weak)                 │
│         We lose big pots (they trap with strong)           │
│         Net: Exploiter profits significantly               │
└────────────────────────────────────────────────────────────┘
```

### Why This Happened

1. **Opponent pool too weak**: Random, call_station, weak historical versions all fold to aggression
2. **No counter-pressure**: No opponent punishes constant betting
3. **Local optimum**: "Always bet" maximizes reward against current opponents

## Solution: Counter-Strategy Opponents

### 1. Trapper Opponent (NEW)

The key missing opponent - specifically designed to exploit hyper-aggressive players:

```python
@jax.jit
def trapper_opponent(
    state: GameState,
    valid_mask: Array,
    rng_key: Array,
    obs: Array,
) -> Array:
    """Trapper opponent - exploits aggressive players.

    Strategy:
    - Strong hands (>0.70): Call/check to trap (let aggro bet into us)
    - Medium hands (0.45-0.70): Call if cheap, otherwise fold
    - Weak hands (<0.45): Fold immediately (don't pay off)

    This punishes "always bet" by:
    - Winning big pots when we have strong hands (they bet, we call)
    - Losing small pots when we have weak hands (we fold early)
    """
    # Get hand strength
    strength = get_hand_strength(state, obs)

    strong = strength > 0.70
    medium = (strength > 0.45) & ~strong

    can_check = valid_mask[:, ACTION_CHECK]
    can_call = valid_mask[:, ACTION_CALL]

    # Strong: always call/check (trap)
    strong_action = jnp.where(can_check, ACTION_CHECK,
                    jnp.where(can_call, ACTION_CALL, ACTION_FOLD))

    # Medium: call small bets, fold to big pressure
    # Check pot odds - fold if facing >50% pot bet
    pot_odds_ok = compute_pot_odds(state) > 0.25
    medium_action = jnp.where(can_check, ACTION_CHECK,
                    jnp.where(can_call & pot_odds_ok, ACTION_CALL, ACTION_FOLD))

    # Weak: fold (don't pay off aggro)
    weak_action = jnp.where(can_check, ACTION_CHECK, ACTION_FOLD)

    return jnp.where(strong, strong_action,
           jnp.where(medium, medium_action, weak_action))
```

**Why this works:**
- Against "always bet" model: Trapper calls with strong hands, model loses big
- Against balanced model: Trapper's passive play gets exploited by value betting
- Forces model to learn: "I can't always bluff - need to check hand strength"

### 2. Rock Opponent (Already Exists)

Very tight player - only plays premium hands. Located in `evaluation/opponents.py`.

**Why it helps:**
- Folds to most aggression (model wins small pots)
- When Rock plays back, they have a monster
- Teaches model: "If tight player raises, I should fold weak hands"

### 3. Existing Opponents

| Opponent | Strategy | What It Teaches |
|----------|----------|-----------------|
| Random | Uniform random | Baseline exploitation |
| Call Station | Never folds | Don't over-bluff |
| TAG | Tight-aggressive | Standard solid play |
| LAG | Loose-aggressive | Handle aggression |
| Rock | Very tight | Respect raises from tight players |
| Historical | Past model versions | Don't regress |

## Solution: Entropy Regularization

Add entropy bonus to PPO loss to prevent action distribution collapse:

```python
# In training/jax_ppo.py

def ppo_loss_with_entropy(
    action_logits,
    actions,
    advantages,
    old_log_probs,
    valid_masks,
    entropy_coef: float = 0.05,  # Higher than default 0.01
):
    """PPO loss with entropy regularization."""

    # Standard PPO loss
    policy_loss = compute_ppo_loss(...)

    # Entropy bonus (encourages action diversity)
    action_probs = jax.nn.softmax(action_logits)
    entropy = -jnp.sum(action_probs * jnp.log(action_probs + 1e-8), axis=-1)
    entropy_loss = -entropy_coef * entropy.mean()

    # Combined loss
    total_loss = policy_loss + entropy_loss

    return total_loss, {"entropy": entropy.mean()}
```

**Why this works:**
- Penalizes peaked distributions (e.g., 100% raise)
- Encourages maintaining diverse action probabilities
- Prevents collapse to degenerate strategies

## Solution: Diverse Opponent Mix

Update opponent mix to include counter-strategy opponents:

```python
# v3.3 opponent mix (anti-exploitation)
historical_opponent_mix = {
    "self": 0.30,           # 30% current self-play
    "historical": 0.25,     # 25% historical checkpoints
    "trapper": 0.15,        # 15% trapper (NEW - punishes aggro)
    "call_station": 0.10,   # 10% call station
    "tag": 0.08,            # 8% tight-aggressive
    "rock": 0.07,           # 7% rock (very tight)
    "random": 0.05,         # 5% random
}
```

**Key changes:**
- Added 15% trapper (critical for breaking aggro habit)
- Added 7% rock (teaches respect for tight raises)
- Reduced self/historical to make room

## Implementation Plan

### Files to Modify

1. **`evaluation/opponents.py`**
   - Add `trapper_opponent()` function
   - Add helper `compute_pot_odds()` for trapper logic

2. **`training/jax_trainer.py`**
   - Update `OPP_TRAPPER = 6` constant
   - Update `sample_opponent_types()` for 7 opponent types
   - Update `get_mixed_opponent_actions()` to handle trapper
   - Update default `historical_opponent_mix`

3. **`training/jax_ppo.py`**
   - Increase entropy coefficient from 0.01 to 0.05
   - Add entropy tracking to metrics

4. **`main.py`**
   - Add `--entropy-coef` CLI option

### Training Command (v3.3)

```bash
nohup uv run python main.py train-rl-jax \
  --model v3.3 \
  --steps 1000000000 \
  --parallel 1536 \
  --historical-selfplay \
  --entropy-coef 0.05 \
  --tensorboard logs/v3.3 \
  --desc "Anti-exploitation training with trapper opponent" \
  > nohup_v3.3.out 2>&1 &
```

## Expected Outcomes

### Metrics to Watch

| Metric | v3.2 (Current) | v3.3 (Target) |
|--------|----------------|---------------|
| Fold rate | 0% | 10-20% |
| Call rate | 0% | 15-25% |
| Raise rate | 83% | 55-70% |
| Action entropy | ~0.3 | ~1.2 |
| Win rate vs Trapper | <40% | >55% |

### Behavioral Changes

1. **Learns to fold**: Weak hands against aggression
2. **Learns to call**: Medium hands for pot odds
3. **Selective aggression**: Raises with strong hands, not everything
4. **Opponent-aware**: Different strategy vs different opponents

## Evaluation

After training, evaluate against all opponent types:

```bash
uv run python main.py evaluate \
  --model v3.3 \
  --opponents random,call_station,tag,lag,rock,trapper \
  --games 1000
```

Target win rates:
- vs Random: >75%
- vs Call Station: >65%
- vs TAG: >55%
- vs LAG: >50%
- vs Rock: >55%
- vs Trapper: >55%  ← Key metric

## Future: Population-Based Training

For even more robust anti-exploitation, consider population-based training:

```
┌─────────────────────────────────────────────────────────────┐
│                 Population of 8 Agents                      │
│                                                             │
│   Agent 1 ←──┐                                              │
│   Agent 2 ←──┤   Each agent plays against                   │
│   Agent 3 ←──┤   random opponents from the                  │
│   Agent 4 ←──┼── population (including itself)              │
│   Agent 5 ←──┤                                              │
│   Agent 6 ←──┤   Natural pressure to not be                 │
│   Agent 7 ←──┤   exploitable (others will learn             │
│   Agent 8 ←──┘   to counter your strategy)                  │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

This creates emergent counter-strategies without manually designing opponents.

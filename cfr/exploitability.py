"""Exploitability measurement for CFR strategies.

Exploitability measures how far a strategy is from Nash equilibrium.
A Nash equilibrium has exploitability = 0.

This module provides:
- Best response computation against a fixed strategy
- Monte Carlo exploitability estimation
"""

from functools import partial
from typing import Tuple

import jax
import jax.numpy as jnp
from jax import Array

from poker_jax.state import GameState, NUM_ACTIONS
from poker_jax.game import reset, step, get_rewards
from poker_jax.state import get_valid_actions_mask as get_valid_actions
from poker_jax.encoding import encode_state_for_current_player as get_observation

from .abstraction import (
    compute_info_key_batch_v2,
    info_key_to_index,
    MAX_INFO_SETS,
    ROUND_PREFLOP,
)


# Observation index for normalized hand strength
HAND_STRENGTH_OFFSET = 104 + 260 + 4 + 2 + 6 + 9  # = 385
NORMALIZED_STRENGTH_IDX = HAND_STRENGTH_OFFSET + 10  # = 395


@jax.jit
def get_strategy_for_state(
    strategy: Array,
    state: GameState,
    obs: Array,
) -> Array:
    """Get strategy probabilities for current game state.

    Args:
        strategy: [MAX_INFO_SETS, NUM_ACTIONS] strategy table
        state: Current game state [N]
        obs: Observations [N, OBS_DIM]

    Returns:
        [N, NUM_ACTIONS] strategy probabilities
    """
    n_games = state.done.shape[0]
    game_idx = jnp.arange(n_games)

    # Get current player's hole cards
    player_idx = state.current_player
    hole_cards = state.hole_cards[game_idx, player_idx, :]

    # Get hand strength from observation
    normalized_strength = obs[:, NORMALIZED_STRENGTH_IDX]

    # Compute info set indices using V2 abstraction
    info_keys = compute_info_key_batch_v2(
        hole_cards=hole_cards,
        street=state.round,
        normalized_strength=normalized_strength,
        pot=state.pot,
        bets=state.bets,
        current_player=state.current_player,
        button=state.button,
        starting_chips=jnp.full(n_games, 200.0),  # Standard starting stack
        action_history=state.action_history,
        history_len=state.history_len,
    )
    info_indices = info_key_to_index(info_keys)

    return strategy[info_indices]


@jax.jit
def compute_action_values(
    state: GameState,
    valid_mask: Array,
    obs: Array,
    strategy: Array,
    rng_key: Array,
    rollout_depth: int = 20,
) -> Array:
    """Estimate action values via Monte Carlo rollout.

    For each valid action, simulates the rest of the game with
    the fixed strategy and returns expected value.

    Args:
        state: Current game state [N]
        valid_mask: [N, NUM_ACTIONS] valid action mask
        obs: [N, OBS_DIM] observations
        strategy: [MAX_INFO_SETS, NUM_ACTIONS] fixed strategy
        rng_key: PRNG key
        rollout_depth: Max steps to simulate

    Returns:
        [N, NUM_ACTIONS] estimated action values for current player
    """
    n_games = state.done.shape[0]
    current_player = state.current_player

    def rollout_action(action_idx: int, key: Array) -> Array:
        """Rollout from taking a specific action."""
        action = jnp.full(n_games, action_idx, dtype=jnp.int32)

        # Take the action
        next_state = step(state, action)
        rewards = get_rewards(next_state)
        cumulative_reward = rewards[jnp.arange(n_games), current_player]

        # Continue rollout with fixed strategy
        def rollout_step(carry, key):
            s, cum_r = carry

            # Get observation and valid actions
            obs_t = get_observation(s)
            valid_t = get_valid_actions(s)

            # Get strategy probabilities
            probs = get_strategy_for_state(strategy, s, obs_t)

            # Mask and renormalize
            masked = jnp.where(valid_t, probs, 0.0)
            total = masked.sum(axis=-1, keepdims=True)
            probs = masked / (total + 1e-9)

            # Uniform fallback
            uniform = valid_t.astype(jnp.float32)
            uniform = uniform / (uniform.sum(axis=-1, keepdims=True) + 1e-9)
            probs = jnp.where(total > 0, probs, uniform)

            # Sample action
            act = jax.random.categorical(key, jnp.log(probs + 1e-9))

            # Step
            next_s = step(s, act)
            r = get_rewards(next_s)

            # Accumulate reward for the player we're evaluating
            new_cum_r = cum_r + r[jnp.arange(n_games), current_player]

            # Only update if not done
            new_cum_r = jnp.where(s.done, cum_r, new_cum_r)
            next_s = jax.tree.map(
                lambda old, new: jnp.where(s.done[:, None] if old.ndim > 1 else s.done, old, new),
                s, next_s
            )

            return (next_s, new_cum_r), None

        keys = jax.random.split(key, rollout_depth)
        (final_state, final_reward), _ = jax.lax.scan(
            rollout_step,
            (next_state, cumulative_reward),
            keys,
        )

        return final_reward

    # Rollout each action
    keys = jax.random.split(rng_key, NUM_ACTIONS)
    action_values = jax.vmap(rollout_action)(jnp.arange(NUM_ACTIONS), keys)

    # Transpose to [N, NUM_ACTIONS]
    action_values = action_values.T

    # Mask invalid actions with large negative value
    action_values = jnp.where(valid_mask, action_values, -1e6)

    return action_values


@partial(jax.jit, static_argnums=(2, 3, 4))
def estimate_exploitability_batch(
    strategy: Array,
    rng_key: Array,
    starting_chips: float = 200.0,
    n_games: int = 1024,
    rollouts_per_state: int = 4,
) -> Tuple[Array, Array]:
    """Estimate exploitability for a batch of random games.

    Exploitability = value of best response - value of current strategy

    For each sampled state:
    1. Compute value of current strategy (via rollout)
    2. Compute value of best response (max over actions)
    3. Gap = BR_value - strategy_value

    Args:
        strategy: [MAX_INFO_SETS, NUM_ACTIONS] strategy table
        rng_key: PRNG key
        starting_chips: Starting chip count
        n_games: Number of games to sample
        rollouts_per_state: Rollouts per state for variance reduction

    Returns:
        (mean_exploitability, std_exploitability) in chips
    """
    key1, key2, key3 = jax.random.split(rng_key, 3)

    # Sample random game states by playing some random actions
    state = reset(key1, n_games, starting_chips)

    # Play random actions to get to interesting states
    def random_step(carry, key):
        s = carry
        valid = get_valid_actions(s)

        # Uniform random action
        uniform = valid.astype(jnp.float32)
        uniform = uniform / (uniform.sum(axis=-1, keepdims=True) + 1e-9)
        action = jax.random.categorical(key, jnp.log(uniform + 1e-9))

        next_s = step(s, action)

        # Don't step if done
        next_s = jax.tree.map(
            lambda old, new: jnp.where(s.done[:, None] if old.ndim > 1 else s.done, old, new),
            s, next_s
        )

        return next_s, None

    # Random prefix (1-10 steps)
    prefix_keys = jax.random.split(key2, 10)
    state, _ = jax.lax.scan(random_step, state, prefix_keys)

    # Filter to non-terminal states
    not_done = ~state.done

    # Get observations and valid actions
    obs = get_observation(state)
    valid_mask = get_valid_actions(state)

    # Compute action values via rollout
    def compute_with_rollout(key):
        return compute_action_values(state, valid_mask, obs, strategy, key)

    # Average over multiple rollouts for variance reduction
    rollout_keys = jax.random.split(key3, rollouts_per_state)
    all_values = jax.vmap(compute_with_rollout)(rollout_keys)  # [rollouts, N, actions]
    action_values = all_values.mean(axis=0)  # [N, actions]

    # Best response value (max over valid actions)
    br_values = action_values.max(axis=-1)  # [N]

    # Current strategy value (expected value under strategy)
    current_probs = get_strategy_for_state(strategy, state, obs)
    masked_probs = jnp.where(valid_mask, current_probs, 0.0)
    total = masked_probs.sum(axis=-1, keepdims=True)
    probs = masked_probs / (total + 1e-9)

    # Uniform fallback
    uniform = valid_mask.astype(jnp.float32)
    uniform = uniform / (uniform.sum(axis=-1, keepdims=True) + 1e-9)
    probs = jnp.where(total > 0, probs, uniform)

    strategy_values = (probs * action_values).sum(axis=-1)  # [N]

    # Exploitability = BR - strategy (how much better BR does)
    exploit_gaps = br_values - strategy_values

    # Only count non-terminal states
    exploit_gaps = jnp.where(not_done, exploit_gaps, 0.0)
    n_valid = not_done.sum()

    mean_exploit = exploit_gaps.sum() / (n_valid + 1e-9)
    var_exploit = ((exploit_gaps - mean_exploit) ** 2 * not_done).sum() / (n_valid + 1e-9)
    std_exploit = jnp.sqrt(var_exploit)

    return mean_exploit, std_exploit


def compute_exploitability(
    strategy: Array,
    rng_key: Array,
    n_samples: int = 10000,
    batch_size: int = 1024,
    starting_chips: float = 200.0,
) -> dict:
    """Compute exploitability estimate with statistics.

    Args:
        strategy: [MAX_INFO_SETS, NUM_ACTIONS] strategy table
        rng_key: PRNG key
        n_samples: Total number of game states to sample
        batch_size: Batch size for parallel computation
        starting_chips: Starting chip count

    Returns:
        Dictionary with exploitability stats:
        - mean: Mean exploitability in chips
        - std: Standard deviation
        - bb100: Exploitability in BB/100 hands
    """
    n_batches = max(1, n_samples // batch_size)

    all_means = []
    all_stds = []

    for i in range(n_batches):
        key, rng_key = jax.random.split(rng_key)
        mean, std = estimate_exploitability_batch(
            strategy, key, starting_chips, batch_size, rollouts_per_state=4
        )
        all_means.append(float(mean))
        all_stds.append(float(std))

    # Combine estimates
    overall_mean = sum(all_means) / len(all_means)
    # Pooled standard deviation
    overall_std = (sum(s**2 for s in all_stds) / len(all_stds)) ** 0.5

    # Convert to BB/100
    # 1 BB = 2 chips (big blind), so exploitability in BB = chips / 2
    # BB/100 = BB per 100 hands
    bb100 = (overall_mean / 2) * 100

    return {
        "mean_chips": overall_mean,
        "std_chips": overall_std,
        "bb100": bb100,
        "n_samples": n_batches * batch_size,
    }


@jax.jit
def compute_strategy_entropy(strategy: Array) -> Array:
    """Compute entropy of strategy (higher = more mixed).

    Args:
        strategy: [MAX_INFO_SETS, NUM_ACTIONS] strategy table

    Returns:
        [MAX_INFO_SETS] entropy values
    """
    # Normalize to ensure valid probabilities
    probs = strategy / (strategy.sum(axis=-1, keepdims=True) + 1e-9)
    probs = jnp.clip(probs, 1e-9, 1.0)

    # H = -sum(p * log(p))
    entropy = -jnp.sum(probs * jnp.log(probs), axis=-1)

    return entropy


def analyze_strategy_quality(strategy: Array) -> dict:
    """Comprehensive analysis of strategy quality.

    Args:
        strategy: [MAX_INFO_SETS, NUM_ACTIONS] strategy table

    Returns:
        Dictionary with quality metrics
    """
    # Count active info sets (non-zero rows)
    row_sums = strategy.sum(axis=-1)
    active_mask = row_sums > 1e-6
    n_active = int(active_mask.sum())

    # Extract active strategies
    active_strats = strategy[active_mask]

    if n_active == 0:
        return {
            "n_active": 0,
            "n_pure": 0,
            "n_mixed": 0,
            "pct_pure": 0.0,
            "mean_entropy": 0.0,
            "max_entropy": 0.0,
        }

    # Normalize active strategies
    active_probs = active_strats / (active_strats.sum(axis=-1, keepdims=True) + 1e-9)

    # Pure vs mixed
    max_probs = active_probs.max(axis=-1)
    is_pure = max_probs > 0.95
    n_pure = int(is_pure.sum())
    n_mixed = n_active - n_pure

    # Entropy analysis
    entropy = compute_strategy_entropy(active_strats)
    mean_entropy = float(entropy.mean())
    max_entropy = float(jnp.log(NUM_ACTIONS))  # Maximum possible entropy

    return {
        "n_active": n_active,
        "n_pure": n_pure,
        "n_mixed": n_mixed,
        "pct_pure": n_pure / n_active * 100,
        "pct_mixed": n_mixed / n_active * 100,
        "mean_entropy": mean_entropy,
        "max_entropy": max_entropy,
        "entropy_ratio": mean_entropy / max_entropy,  # 0-1, higher = more mixed
    }


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python -m cfr.exploitability <strategy_path>")
        sys.exit(1)

    strategy_path = sys.argv[1]
    print(f"Loading strategy from {strategy_path}")
    strategy = jnp.load(strategy_path)
    print(f"Strategy shape: {strategy.shape}")

    # Analyze strategy quality
    print("\n=== Strategy Quality Analysis ===")
    quality = analyze_strategy_quality(strategy)
    for k, v in quality.items():
        if isinstance(v, float):
            print(f"  {k}: {v:.4f}")
        else:
            print(f"  {k}: {v}")

    # Compute exploitability
    print("\n=== Exploitability Estimation ===")
    print("Computing (this may take a minute)...")

    rng_key = jax.random.key(42)
    exploit = compute_exploitability(
        strategy, rng_key,
        n_samples=5000,
        batch_size=512,
    )

    print(f"  Mean exploitability: {exploit['mean_chips']:.4f} chips")
    print(f"  Std: {exploit['std_chips']:.4f} chips")
    print(f"  Exploitability: {exploit['bb100']:.2f} BB/100")
    print(f"  Samples: {exploit['n_samples']}")
